{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Fast-RCNN model directly from python\n",
    "\n",
    "This notebook demonstrates eveluation of a model trained by the Fast-RCNN implementation of CNTK.\n",
    "\n",
    "For a full description of the model and the algorithm, please see the following tutorial: https://github.com/Microsoft/CNTK/wiki/Object-Detection-using-Fast-R-CNN \n",
    "\n",
    "Below, you will see sample code for:\n",
    "1. Preparing the input data for the network (including image size adjustments)\n",
    "2. Evaluation of the input data using the model\n",
    "3. Processing the evluation result and presenting the selected regions back on the image\n",
    "\n",
    "Before running this notebook, please make sure that:\n",
    "<ol>\n",
    "<li>You have version >= 2.0 of CNTK installed. Installation instructions are available here: https://github.com/Microsoft/CNTK/wiki/Setup-CNTK-on-your-machine\n",
    "\n",
    "The current assumption is that you have CNTK installed on a windows machine under \"c:\\local\\cntk\". You can change the path for the CNTK by changing the \"cntk_base_path\" variable that is defined below.</li>\n",
    "\n",
    "<li>You trained the Fast R-CNN model example for the groecry dataset run according to the instructions in the [tutorial above](https://github.com/Microsoft/CNTK/wiki/Object-Detection-using-Fast-R-CNN).<br>\n",
    "**Important**: Please note that this example works with the Brain Script model that supports version 2.0 of CNTK.\n",
    "To use the brainscript model, make sure to use following fastrnn.cntk configuration file: https://github.com/Microsoft/CNTK/blob/pkranen/frcnPythonApi/Examples/Image/Detection/FastRCNN/fastrcnn.cntk\n",
    "\n",
    "The configuration uses the BrainScript version for the AlexNet. Prior to running the A2_RunCntk.py script, you should download the model from: https://www.cntk.ai/Models/AlexNet/AlexNetBS.model and place it under the: \"<i>C:\\local\\cntk\\Examples\\Image\\PretrainedModels</i>\" directory.</li>\n",
    "\n",
    "<li>This notebook uses the CNTK python APIs and should be run from the CNTK python enviornment. The enviornment can be started by running the script: \"c:\\local\\cntk\\Scripts\\cntkpy34.py\".</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# the above line enable us to draw the images inside the notebooks\n",
    "\n",
    "# path to the CNTK installation\n",
    "cntk_base_path = r\"C:\\local\\cntk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cntk import load_model\n",
    "from os.path import join\n",
    "frcnn_model = load_model(join(cntk_base_path, r\"Examples/Image/Detection/FastRCNN/proc/grocery_2000/cntkFiles/Output/Fast-RCNN.model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image and convert it to the network format\n",
    "\n",
    "The image is loaded using OpenCV, and then resized according to the network input dimensions.\n",
    "\n",
    "When resizing, we preserve scale and pad the border areas with a constant value (114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_height = 1000\n",
    "image_width = 1000 \n",
    "\n",
    "def resize_and_pad(img, width, height, pad_value=114):\n",
    "    # port of the c++ code from CNTK: https://github.com/Microsoft/CNTK/blob/f686879b654285d06d75c69ee266e9d4b7b87bc4/Source/Readers/ImageReader/ImageTransformers.cpp#L316\n",
    "    img_width = len(img[0])\n",
    "    img_height = len(img)\n",
    "    \n",
    "    scale_w = img_width > img_height\n",
    "    \n",
    "    target_w = width\n",
    "    target_h = height\n",
    "    \n",
    "    if scale_w:\n",
    "        target_h = int(np.round(img_height * float(width) / float(img_width)))\n",
    "    else:\n",
    "        target_w = int(np.round(img_width * float(height) / float(img_height)))\n",
    "        \n",
    "    resized = cv2.resize(img, (target_w, target_h), 0, 0, interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    top = int(max(0, np.round((height - target_h) / 2)))\n",
    "    left = int(max(0, np.round((width - target_w) / 2)))\n",
    "    \n",
    "    bottom = height - top - target_h\n",
    "    right = width - left - target_w\n",
    "    \n",
    "    resized_with_pad = cv2.copyMakeBorder(resized, top, bottom, left, right, \n",
    "                                          cv2.BORDER_CONSTANT, value=[pad_value, pad_value, pad_value])\n",
    "        \n",
    "    #tranpose(2,0,1) converts the image to the HWC format which CNTK accepts\n",
    "    model_arg_rep = np.ascontiguousarray(np.array(resized_with_pad, dtype=np.float32).transpose(2,0,1))\n",
    "    \n",
    "    return resized_with_pad, model_arg_rep\n",
    "\n",
    "def load_image_and_scale(image_path, width, height, pad_value=114):\n",
    "    img = cv2.imread(image_path)\n",
    "    return resize_and_pad(img, width, height, pad_value), img\n",
    "\n",
    "test_image_path = join(cntk_base_path, r\"Examples/Image/DataSets/grocery/testImages/WIN_20160803_11_28_42_Pro.jpg\")\n",
    "(test_img, test_img_model_arg), original_img = load_image_and_scale(test_image_path, image_width, image_height)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ROIs for testing\n",
    "\n",
    "Current ROI data is taken from the example test file. \n",
    "ROIs can be produced using a sliding window/selective search. The given ROIs are in the format of [x,y,w,h] and in the coordinates of the scaled and padded image.\n",
    "\n",
    "The ROIs are padded with regions of [0,0,0,0] at the end to match the 2000 ROIs model.\n",
    "\n",
    "We use a helper function to find where the pading start and store the length of the actual list without the padding in order to use it later in the evaluation.\n",
    "\n",
    "As a final step, we also convert the coordinate of the ROIs back to the coordinates of the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#read the test rois from file and convert into a float matrix of 2000 X 4\n",
    "with open('./test_rois.txt', 'r') as rois_file:\n",
    "    rois_sample=rois_file.read().replace('\\n', '')\n",
    "rr = rois_sample.split(\" \")\n",
    "test_rois = np.array([[float(rr[i*4]), float(rr[i*4+1]), float(rr[i*4+2]), float(rr[i*4+3])] for i in range(int(len(rr)/4))])\n",
    "\n",
    "# find where the padding begins in case we read the rois from a file or use some test data\n",
    "# in case we generate the rois in the script we should know this in advance\n",
    "def find_roi_padding(rois):\n",
    "    # we can probably make this more efficient if we use binary search, etc.\n",
    "    # anyway its something that we should know when we run the evaluation\n",
    "    for i in range(len(rois)):\n",
    "        roi = rois[i]\n",
    "        if (roi[0] == 0.0 and roi[1] == 0 and roi[2] == 0.0 and roi[3] == 0.0):\n",
    "            return i\n",
    "    return len(rois)+1\n",
    "\n",
    "roi_padding_index = find_roi_padding(test_rois)\n",
    "\n",
    "## Convert the ROIs back to the image coordinates\n",
    "def to_original_rois(rois, img_width, img_height, dest_width, dest_height):\n",
    "    scale_w = img_width > img_height\n",
    "    \n",
    "    target_h = dest_height\n",
    "    target_w = dest_width\n",
    "    height_ratio = 1\n",
    "    width_ratio = 1\n",
    "    if scale_w:\n",
    "        height_ratio = float(dest_width) / float(img_width)\n",
    "        target_h = int(np.round(img_height * height_ratio))\n",
    "    else:\n",
    "        width_ratio = float(dest_height) / float(img_height)\n",
    "        target_w = int(np.round(img_width * width_ratio))\n",
    "    \n",
    "    top = max(0, np.round((dest_height - target_h) / 2)) / float(dest_width)\n",
    "    left = max(0, np.round((dest_width - target_w) / 2)) / float(dest_height)\n",
    "    \n",
    "    original_rois = []\n",
    "    for r in rois:\n",
    "        x1 = int((r[0] - left) * dest_width / width_ratio)\n",
    "        y1 = int((r[1] - top)*img_height)\n",
    "        x2 = x1 + int(r[2] * dest_width / width_ratio)\n",
    "        y2 = y1 + int(img_height * r[3]) \n",
    "        original_rois.append([x1, y1, x2, y2])\n",
    "    return np.array(original_rois)\n",
    "        \n",
    "original_rois = to_original_rois(test_rois, len(original_img[0]), len(original_img), 1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the sample\n",
    "Prepare the data to be in CNTK's expected arguments format and run it through the model\n",
    "\n",
    "Process the result by trimming the padded rois part, and calculate the predicted labels and probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\local\\cntk\\Examples\\Image\\Detection\\FastRCNN\")\n",
    "from cntk_helpers import softmax2D\n",
    "\n",
    "# a dummy variable for labels the will be given as an input to the network but will be ignored\n",
    "dummy_labels = np.zeros((2000,17))\n",
    "\n",
    "# prepare the arguments\n",
    "arguments = {\n",
    "    frcnn_model.arguments[1]: [test_img_model_arg],\n",
    "    frcnn_model.arguments[0]: [test_rois],\n",
    "    frcnn_model.arguments[2] : [dummy_labels]\n",
    "}\n",
    "\n",
    "# run it through the model\n",
    "output = frcnn_model.eval(arguments)\n",
    "\n",
    "# take just the relevant part and cast to float64 to prevent overflow when doing softmax\n",
    "rois_values = output[frcnn_model.outputs[2]][0][0][:roi_padding_index].astype(np.float64)\n",
    "\n",
    "# get the prediction for each roi by taking the index with the maximal value in each row \n",
    "rois_labels_predictions = np.argmax(rois_values, axis=1)\n",
    "\n",
    "# calculate the probabilities using softmax\n",
    "rois_probs = softmax2D(rois_values) \n",
    "\n",
    "# print the number of ROIs that were detected as non-background\n",
    "print(\"Number of detections: %d\"%np.sum(rois_labels_predictions > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge overlapping regions using Non Maxima Supression\n",
    "The code below merged overlapping regions that were detected using Non-Maxima-Supression algorithm that is implemented in the cntk_helpers module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cntk_helpers import applyNonMaximaSuppression\n",
    "nms_threshold = 0.1\n",
    "non_padded_rois = test_rois[:roi_padding_index]\n",
    "max_probs = np.amax(rois_probs, axis=1).tolist()\n",
    "rois_prediction_indices = applyNonMaximaSuppression(nms_threshold, rois_labels_predictions, max_probs, non_padded_rois)\n",
    "print(\"Indices of seletected regions:\",rois_prediction_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results\n",
    "\n",
    "As a final step, we use the OpenCV **rectangle** library in order to draw the seleted regions on the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rois_with_prediction = test_rois[rois_prediction_indices]\n",
    "rois_prediction_labels = rois_labels_predictions[rois_prediction_indices]\n",
    "rois_predicion_scores = rois_values[rois_prediction_indices]\n",
    "original_rois_predictions = original_rois[rois_prediction_indices]\n",
    "\n",
    "original_img_cpy = original_img.copy()\n",
    "for roi in original_rois_predictions:\n",
    "    (x1,y1,x2,y2) = roi\n",
    "    cv2.rectangle(original_img_cpy, (x1, y1), (x2, y2), (0, 255, 0), 5)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(original_img_cpy, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
